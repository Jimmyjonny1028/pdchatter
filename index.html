# File: worker.py (for your Local PC)
import asyncio
import websockets
import json
import fitz
import numpy as np
from PIL import Image
import pytesseract
import os
from dotenv import load_dotenv
import google.generativeai as genai
import re
import base64
import torch
import cv2
from io import BytesIO
import sys
from omegaconf import OmegaConf

# --- LaMa Model Integration ---
LAMA_PROJECT_PATH = "C:/Users/User/lama" 
sys.path.append(LAMA_PROJECT_PATH)
try:
    from saic_vision.training.trainers import load_checkpoint
    from saic_vision.evaluation.utils import move_to_device, load_image
    from saic_vision.models.inpainting import InpaintingModel
    print("Successfully imported LaMa project files.")
except ImportError as e:
    print(f"Could not import LaMa project files. Make sure the LAMA_PROJECT_PATH is correct. Error: {e}")
    InpaintingModel = None

# --- CONFIGURATION & INITIALIZATION ---
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY not found in .env file.")
genai.configure(api_key=GOOGLE_API_KEY)
try:
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
except Exception:
    pass

print("Initializing Gemini model...")
llm_model_name = 'gemini-1.5-flash-latest'
llm_model = genai.GenerativeModel(llm_model_name)
print("Gemini model initialized.")

# --- Load the LaMa Watermark Removal Model ---
inpainting_model = None
if InpaintingModel:
    try:
        print("Loading LaMa watermark removal model...")
        model_path = "C:/Users/User/lama/models/best.ckpt"
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        train_config_str = "model:\n  kind: default\n  _target_: saic_vision.models.inpainting.InpaintingModel"
        train_config = OmegaConf.create(train_config_str)
        
        inpainting_model = InpaintingModel(train_config.model)
        load_checkpoint(inpainting_model, model_path, map_location='cpu', strict=False)
        inpainting_model.to(device)
        inpainting_model.eval()
        print(f"LaMa model '{model_path}' loaded successfully to device: {device}")
    except Exception as e:
        print(f"Error loading LaMa model: {e}")
        inpainting_model = None

# --- File-based chat history management ---
CHAT_HISTORY_DIR = "chat_histories"
PDF_STORAGE_DIR = "uploaded_pdfs"
os.makedirs(CHAT_HISTORY_DIR, exist_ok=True)
os.makedirs(PDF_STORAGE_DIR, exist_ok=True)
user_sessions = {}

# --- CORE AI FUNCTIONS ---
def process_pdf(pdf_content: bytes, filename: str, user_id: str):
    global user_sessions
    try:
        session_key = f"{user_id}_{filename}"
        user_sessions[user_id] = {"session_key": session_key, "full_text": ""}

        pdf_path = os.path.join(PDF_STORAGE_DIR, f"{session_key}.pdf")
        with open(pdf_path, "wb") as f:
            f.write(pdf_content)

        doc = fitz.open(stream=pdf_content, filetype="pdf")
        full_text = "".join(page.get_text("text") for page in doc)
        if len(full_text.strip()) < 100:
            full_text = ""
            for page in doc:
                pix = page.get_pixmap(dpi=300)
                img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                full_text += pytesseract.image_to_string(img) + "\n"
        doc.close()

        if not full_text.strip():
            return {"type": "error", "data": "Could not extract any text from the PDF.", "user_id": user_id}

        user_sessions[user_id]["full_text"] = full_text
        
        chat_history_path = os.path.join(CHAT_HISTORY_DIR, f"{session_key}.json")
        with open(chat_history_path, 'w') as f:
            json.dump([], f)

        return {"type": "status", "data": "PDF processed successfully. Ready for questions.", "user_id": user_id}
    except Exception as e:
        return {"type": "error", "data": str(e), "user_id": user_id}

def ask_question(question: str, user_id: str):
    global user_sessions
    try:
        session = user_sessions.get(user_id)
        if not session or not session.get("full_text"):
            return {"type": "error", "data": "No active session. Please upload a PDF first.", "user_id": user_id}

        prompt = f"""
        You are a helpful and precise assistant...
        ---
        {session['full_text']}
        ---
        **User Question:**
        {question}
        **Answer:**
        """
        
        generation_config = genai.types.GenerationConfig(max_output_tokens=2048, temperature=0.25)
        response = llm_model.generate_content(prompt, generation_config=generation_config)
        
        answer = response.text.strip() if response.parts else "The model did not provide a response."
        
        chat_history_path = os.path.join(CHAT_HISTORY_DIR, f"{session['session_key']}.json")
        with open(chat_history_path, 'r+') as f:
            history = json.load(f)
            history.append({"sender": "user", "text": question})
            history.append({"sender": "ai", "text": answer})
            f.seek(0)
            json.dump(history, f)

        return {"type": "answer", "data": answer, "user_id": user_id}
    except Exception as e:
        return {"type": "error", "data": str(e), "user_id": user_id}

def list_chats(user_id: str):
    user_files = [f for f in os.listdir(CHAT_HISTORY_DIR) if f.startswith(user_id) and f.endswith('.json')]
    chat_names = [f.replace(f"{user_id}_", "").replace(".json", "") for f in user_files]
    return {"type": "chat_list", "data": chat_names, "user_id": user_id}

def load_chat(chat_name: str, user_id: str):
    session_key = f"{user_id}_{chat_name}"
    chat_file_path = os.path.join(CHAT_HISTORY_DIR, f"{session_key}.json")
    pdf_file_path = os.path.join(PDF_STORAGE_DIR, f"{session_key}.pdf")

    if os.path.exists(chat_file_path) and os.path.exists(pdf_file_path):
        with open(chat_file_path, 'r') as f:
            history = json.load(f)
        
        with open(pdf_file_path, 'rb') as f:
            pdf_data = f.read()
            pdf_base64 = base64.b64encode(pdf_data).decode('utf-8')

        process_pdf(pdf_data, chat_name, user_id)

        return {
            "type": "load_chat", 
            "data": { "history": history, "pdf_data": pdf_base64, "pdf_name": chat_name }, 
            "user_id": user_id
        }
    return {"type": "error", "data": "Chat history or PDF file not found.", "user_id": user_id}

def auto_detect_watermark(image: np.ndarray):
    print("Auto-detecting watermark...")
    mask = np.zeros(image.shape[:2], dtype=np.uint8)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)
    kernel = np.ones((15,15), np.uint8)
    mask = cv2.dilate(mask, kernel, iterations=1)
    return mask

def remove_watermark_auto(image_data: bytes):
    if not inpainting_model:
        raise Exception("LaMa inpainting model is not loaded.")

    print("Performing automatic watermark removal...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    image_np = np.frombuffer(image_data, np.uint8)
    image = cv2.imdecode(image_np, cv2.IMREAD_COLOR)
    
    mask = auto_detect_watermark(image)
    
    cv2.imwrite("temp_image.png", image)
    cv2.imwrite("temp_mask.png", mask)

    batch = {}
    batch['image'] = load_image("temp_image.png", mode='RGB')
    batch['mask'] = load_image("temp_mask.png", mode='L')
    
    for k in batch:
        if isinstance(batch[k], np.ndarray):
            batch[k] = torch.from_numpy(batch[k]).permute(2, 0, 1).float()
    
    batch['mask'] = (batch['mask'] > 0) * 1
    batch = move_to_device(batch, device)

    with torch.no_grad():
        batch = inpainting_model(batch)
    
    inpainted_image_np = batch['inpainted'][0].permute(1, 2, 0).cpu().numpy()
    inpainted_image_np = np.clip(inpainted_image_np * 255, 0, 255).astype('uint8')
    
    _, buffer = cv2.imencode('.png', cv2.cvtColor(inpainted_image_np, cv2.COLOR_RGB_BGR))
    
    os.remove("temp_image.png")
    os.remove("temp_mask.png")
    
    return base64.b64encode(buffer).decode('utf-8')


# --- WEBSOCKET CLIENT LOGIC ---
async def handle_task(websocket, task):
    user_id = task.get("user_id")
    if not user_id: return

    result = None
    loop = asyncio.get_running_loop()

    try:
        if task['type'] == 'upload':
            pdf_content = base64.b64decode(task['data'])
            result = await loop.run_in_executor(None, process_pdf, pdf_content, task['filename'], user_id)
        elif task['type'] == 'ask':
            result = await loop.run_in_executor(None, ask_question, task['data'], user_id)
        elif task['type'] == 'list_chats':
            result = await loop.run_in_executor(None, list_chats, user_id)
        elif task['type'] == 'load_chat':
            result = await loop.run_in_executor(None, load_chat, task['data'], user_id)
        elif task['type'] == 'remove_watermark_auto':
            image_content = base64.b64decode(task['image'])
            processed_image_b64 = await loop.run_in_executor(None, remove_watermark_auto, image_content)
            result = {"type": "watermark_removed", "data": processed_image_b64, "user_id": user_id}

        if result:
            await websocket.send(json.dumps(result))
    except Exception as e:
        error_result = {"type": "error", "data": f"Failed to handle task: {e}", "user_id": user_id}
        await websocket.send(json.dumps(error_result))

async def listen_to_server():
    uri = "wss://chatpdf-server-shtq.onrender.com/ws/worker"
    while True:
        try:
            async with websockets.connect(uri, max_size=None) as websocket:
                print("Connected to Render server as a worker.")
                while True:
                    message = await websocket.recv()
                    task = json.loads(message)
                    asyncio.create_task(handle_task(websocket, task))
        except Exception as e:
            print(f"Connection error: {e}. Retrying in 5 seconds...")
            await asyncio.sleep(5)

if __name__ == "__main__":
    asyncio.run(listen_to_server())
